---
title: 'yeet'
date: 2023-12-02
read_time: false
permalink: /posts/2023/12/dc/
tags:
  - computing
---

The phrase "artificial intelligence" has long been a pet peeve of myself and many of my colleagues. 

To make matters worse, AGI is now being co-opted to mean "improved AI" or essentially ChaptGPT 2.0 ([Q*](https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/). I look forward to the unveiling of Ultra Souped Up Amazing Most Honorable AGI in 2030...

In the meantime, however, I haven't found a better, more accurate description of 2010s and 2020s-era deep learning craze than "differentiable computing," a definition which takes nothing away from the amazing engineering feat of deep learning nor overpromises on capabilities given that we are unlikely to agree on what defines "intelligence" any time soon. "AI" probably made sense in the days of Rosenblatt's perceptron when we simply had no idea of how difficult "true" AI would be. We could afford a sense of optimism. 

But the goalposts kept shifting past the 1950s. As smarter people than I have already said, when it works, it's no longer "AI." Chess is the model of true intellect...until our computers grow large enough and an undergraduate can program a world-class chess solver. Then it's language, then it's XYZ, and so forth. No single task or algorithm is likely to be able to define "intelligence."

So being pedantic and rejecting "AI" makes sense in the absence of definition. We need to know where we are if we have any chance of getting to human-level generality and competence. 

First, let's disqualify the second best title I could come up with (perhaps you have others). 

*Computational statistics.* 

Certainly I believe this accurately encapuslates a large swath of ML/AI. Statistical models, traditionally speaking, are of a general form "y(x) = f(x) + eps," where "f(x)" is some underlying "true" or deterministic generating function and "eps" is some noise model. By making some assumptions on the noise or underlying model, we can analytically determine what statisticians care about most: does the estimator converge asymptotically? Is the estimator mostly clustered around the true answer? Is it consistent/unbiased/etc.? But deep learning is mostly indifferent to these questions at present (not even perhaps out of disinterest but of analytical difficulty, though, shameless plug, my [previous lab](https://crisp.seas.harvard.edu/) is working on this). And "computational" at this point is too vague to be helpful. It captures the algorithmic spirit of the 2020s "AI." But when paired with a discipline whose most useful purpose *is* modeling, "computational statistics" really just sounds like the slight oxymoron "algorithmic models," which to me sounds like an interated procedure to find an estimate of parameters of an assumed probability distribution. 

